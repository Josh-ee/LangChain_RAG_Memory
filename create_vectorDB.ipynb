{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Creating the Vector Database with ChromaDB and Hugging Face Embeddings\n",
    "**Introduction:**  \n",
    "In this part, we will create a vector database using Chroma DB to store embeddings generated by Hugging Face's embedding models. This vector database will serve as the foundation for the retrieval component of our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are the necessary libraries, uncomment the ones you need:\n",
    "# !pip install langchain\n",
    "# !pip install chromadb\n",
    "# !pip install arxiv\n",
    "# !pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import arxiv\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download an example PDF from arXiv\n",
    "For this RAG example we are using the Language Models are Few-Shot Learners paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Models are Few-Shot Learners\n"
     ]
    }
   ],
   "source": [
    "client = arxiv.Client()\n",
    "search = arxiv.Search(id_list=['2005.14165'])\n",
    "\n",
    "paper = next(arxiv.Client().results(search))\n",
    "print(paper.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download the PDF locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = paper.download_pdf() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert the PDF to LangChain Documents\n",
    "For this example we will be using the Document format.\n",
    "This allows us to include the page_content and pass our metadata which is uses for citing sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages 75\n"
     ]
    }
   ],
   "source": [
    "reader = PdfReader(path)\n",
    "doc = []\n",
    "for idx, page in enumerate(reader.pages):\n",
    "    doc.append(Document(page_content=page.extract_text(), metadata={'source': f'{paper.title}', 'page':f'{idx+1}'}))\n",
    "\n",
    "print(f'Number of pages {len(doc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare the documents by splitting the data \n",
    "Now we will split the 75 pages into chucks to be vectorized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 974 chunks\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=15)\n",
    "texts = text_splitter.split_documents(doc)\n",
    "\n",
    "print(f'Split into {len(texts)} chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Device:  \n",
    "If you are using a Mac or an Nvidia GPU and installed PyTorch correctly the below will use the correct device  \n",
    "Otherwise it will default to using the CPU\n",
    "\n",
    "For details on how to install PyTorch for CUDA see the [Get Started page](https://pytorch.org/get-started/locally/)  \n",
    "If you are not using CUDA with an Nvidia GPU you can uncomment the line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch for Mac or Windows PC without Nvidia GPU \n",
    "# !pip install torch torchvision torchaudio \n",
    "\n",
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# Detect hardware acceleration device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load embedding model:\n",
    "A good place to start when choosing and embedding model is the [MTEB English Leaderboard](https://huggingface.co/BAAI/bge-small-en)\n",
    "\n",
    "At time of writing, the [BAAI/bge-small-en-v1.5'model](https://huggingface.co/spaces/mteb/leaderboard) is the best small model according to the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BAAI/bge-small-en-v1.5 from HuggingFace\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = 'BAAI/bge-small-en-v1.5'  # Using open source embedding model\n",
    "\n",
    "embedding_function = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True} #normalizes the vectors\n",
    ")\n",
    "print(f'Loaded {model_name} from HuggingFace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create and store the Vector DB\n",
    "* This will use the bge-small-en embeddings model to embed our chunked text into vectors\n",
    "* Then save those vectors into a ChromaDB named \"LC_VectorDB\" \n",
    "\n",
    "**Note**: If a DB with that name already exists, it will append, otherwise it creates it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB write complete!\n"
     ]
    }
   ],
   "source": [
    "persist_directory = 'LC_VectorDB' # Name of the DB\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=doc,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=persist_directory # This line saves the db to disk\n",
    "    )\n",
    "print(\"DB write complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
