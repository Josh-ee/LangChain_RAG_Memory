{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Utilizing the Vector Database with an Open Source LLM Model\n",
    "**Introduction:**  \n",
    "In this part, we will utilized the vectorDB we created in Part 1. To answer questions based on the documents inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install cTransformers CPU:\n",
    "# !pip install ctransformers\n",
    "\n",
    "# Install cTransformers for MacOS:\n",
    "# !CT_METAL=1 pip install ctransformers --no-binary ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Detect hardware acceleration device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    gpu_layers = 50\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    gpu_layers = 1\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    gpu_layers = 0\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Foundational LLM and ask a question\n",
    "Import the Foundation model form HuggingFace  \n",
    "* If this is your first time it can take up to 10 min\n",
    "* Currently using GGUF version of [Mistral-11B-OmniMix](https://huggingface.co/TheBloke/Mistral-11B-OmniMix-GGUF) with 4-bit Quantization \n",
    "* Hyperparams are set in the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f961927ea67d4057aac164b1504e4693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b79553445c4e77a2921726f112c51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'gpu_layers': gpu_layers,  \n",
    "    'temperature': 0.1,\n",
    "    'top_p': 0.9,\n",
    "    'context_length': 8000,\n",
    "    'max_new_tokens': 256,\n",
    "    'repetition_penalty': 1.2,\n",
    "    'reset': True\n",
    "}\n",
    "\n",
    "llm = CTransformers(model='TheBloke/Mistral-11B-OmniMix-GGUF', model_file='mistral-11b-omnimix-bf16.Q4_K_M.gguf', callbacks=[StreamingStdOutCallbackHandler()], config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Prompt:\n",
    "* The Default prompt is the prompt that the user's {question} is injected into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = \"\"\"\n",
    "    You are an AI assistant who is always happy and helpful.\n",
    "    Your answers must be appropriate for a 1st grade classroom, so no controversial topics or answers.\n",
    "    Please answer the following user question:\n",
    "    {question}\n",
    "\n",
    "    Please answer that question thinking step by step\n",
    "    Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Logic Question\n",
    "No RAG Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1) First we have to count the number of birds initially. We know that there were three birds in the nest. So, we can write this as:\n",
      "      - 3 birds (initial)\n",
      "\n",
      "    2) Then two of these birds fly away. This means that we need to subtract two from our initial number of birds.\n",
      "      - 3 birds - 2 = 1 bird\n",
      "\n",
      "    3) Finally, three eggs hatch and become new birds. We can add this to the remaining bird in step 2:\n",
      "      - 1 bird + 3 = 4 birds (final)\n",
      "\n",
      "So, after all these events, there are now four birds!"
     ]
    }
   ],
   "source": [
    "# The full prompt is returned when the users question is combined with the default prompt\n",
    "full_prompt = PromptTemplate(template=default_prompt, input_variables=['question'])\n",
    "\n",
    "llm_chain = LLMChain(prompt=full_prompt, llm=llm) \n",
    "\n",
    "# This is the users question, when you type into ChatGPT this is what you are filling out\n",
    "user_question = 'There are 3 birds in a nest, 2 fly away and then 3 eggs hatch, how many birds are there now?'\n",
    "\n",
    "response = llm_chain.run(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use the LLM with RAG from LC_VectorDB\n",
    "For RAG you need two models\n",
    "* A LLM model (loaded above)\n",
    "* A Embedding model, to embed the user question into a vetor for the vetor Data Base (DB) Search\n",
    "* Since we used the BGE small model in the creation of the DB, we **must** import that same embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma is an open source vector DB\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Choose the same embedding model that used in the creation of the vector DB\n",
    "# - I used the Bge base model so we must import that embedding\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model loaded: BAAI/bge-small-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "# Choose the same embedding model that used in the creation of the vector DB\n",
    "model_name = 'BAAI/bge-small-en-v1.5'  # Using open source embedding model\n",
    "\n",
    "embedding_function = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True} #normalizes the vectors\n",
    ")\n",
    "\n",
    "print(f'Embedding Model loaded: {model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Database loaded: LC_VectorDB\n"
     ]
    }
   ],
   "source": [
    "vector_db_name = 'LC_VectorDB'\n",
    "\n",
    "vectorDB = Chroma(persist_directory=vector_db_name, embedding_function=embedding_function)\n",
    "\n",
    "# k is the number of documents to use: aka use the top 2 most relevant docs\n",
    "retriever = vectorDB.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "print(f'Vector Database loaded: {vector_db_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting for RAG\n",
    "Order of operations:\n",
    "1. The user's question is turned into a vector by the Embedding Model\n",
    "2. That question vector is used to find similar vectors in the Vector Database\n",
    "3. The best \"k\" matches are returned and stuffed into the default prompt where it says {summaries}\n",
    "4. The full prompt with the summaries and user question is passed to the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain \n",
    "\n",
    "# Need a new default prompt that includes the summaries (the data retrieved by RAG)\n",
    "default_prompt_with_context = (\n",
    "    \"\"\"\n",
    "    You are a \"PaperBot\", an AI assistant for answering questions about a arXiv paper. Assume all questions you receive are about this paper.\n",
    "    Please limit your answers to the information provided in the \"Context:\"\n",
    "\n",
    "    Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Context: {summaries}\n",
    "\n",
    "    Use that context to answer the following question about the paper.\n",
    "    Keep your answer short and concise. Do not ramble!\n",
    "    Question: {question}\n",
    "    Answer: \"\"\")\n",
    "\n",
    "\n",
    "chain_type_kwargs={\n",
    "        'prompt': PromptTemplate(\n",
    "            template=default_prompt_with_context,\n",
    "            input_variables=['summaries', 'question'],\n",
    "        ),\n",
    "    }\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff', # stuff means that the context is \"stuffed\" into the context\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True, # This returns the sources used by RAG\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query with RAG\n",
    "Now we will ask a question and the following steps will happen:\n",
    "1. User question is turned into a vector \n",
    "2. That question vector is then compared to the vectors in our VectorDB\n",
    "3. The page_context of best \"k\" matches are returned as \"summaries\" \n",
    "4. We then pass the summaries and non vectorized user question into the default_prompt_with_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    In this paper, zero-shot is described as a setting where \"the model is only given a natural language instruction describing the task\". This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations. However, it also poses challenges due to ambiguity in instructions and lack of prior examples.\n",
      "\n",
      "Sources:\n",
      "  Language Models are Few-Shot Learners, page 7\n",
      "  Language Models are Few-Shot Learners, page 60\n"
     ]
    }
   ],
   "source": [
    "# Now the user question will first be passed into RAG to find relevant info \n",
    "user_question = 'How did they describe zero-shot?'\n",
    "\n",
    "llm_response = chain({'question': user_question})\n",
    "\n",
    "print('\\n\\nSources:')\n",
    "for document in llm_response['source_documents']:  \n",
    "    print(' ', document.metadata['source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional ﬁne-tuning . The panels above show\\nfour methods for performing a task with a language model – ﬁne-tuning is the traditional method, whereas zero-, one-,\\nand few-shot, which we study in this work, require the model to perform the task with only forward passes at test\\ntime. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task\\ndescriptions, examples and prompts can be found in Appendix G.\\n•Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given\\na natural language instruction describing the task. This method provides maximum convenience, potential for\\nrobustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of\\npre-training data), but is also the most challenging setting. In some cases it may even be difﬁcult for humans\\nto understand the format of the task without prior examples, so this setting is in some cases “unfairly hard”.\\nFor example, if someone is asked to “make a table of world records for the 200m dash”, this request can be\\nambiguous, as it may not be clear exactly what format the table should have or what should be included (and\\neven with careful clariﬁcation, understanding precisely what is desired can be difﬁcult). Nevertheless, for at\\nleast some settings zero-shot is closest to how humans perform tasks – for example, in the translation example\\nin Figure 2.1, a human would likely know what to do from just the text instruction.\\nFigure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on\\nzero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different\\nproblem settings which offer a varying trade-off between performance on speciﬁc benchmarks and sample efﬁciency.\\nWe especially highlight the few-shot results as many of them are only slightly behind state-of-the-art ﬁne-tuned models.\\nUltimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance,\\nand are important targets for future work.\\nSections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses\\nthe details of how we do few-shot, one-shot, and zero-shot evaluations.\\n7', metadata={'source': 'Language Models are Few-Shot Learners, page 7'}), Document(page_content=\"Context!Q: What school did burne hogarth establish?\\nA:\\nTarget Completion !School of Visual Arts\\nFigure G.35: Formatted dataset example for WebQA\\nContext!Keinesfalls d\\x7f urfen diese f\\x7f ur den kommerziellen Gebrauch verwendet werden.\\n=\\nTarget Completion !In no case may they be used for commercial purposes.\\nFigure G.36: Formatted dataset example for De !En. This is the format for one- and few-shot learning, for this and\\nother langauge tasks, the format for zero-shot learning is “Q: What is the flanguagegtranslation offsentencegA:\\nftranslationg.”\\nContext!In no case may they be used for commercial purposes. =\\nTarget Completion !Keinesfalls d\\x7f urfen diese f\\x7f ur den kommerziellen Gebrauch verwendet werden.\\nFigure G.37: Formatted dataset example for En !De\\nContext!Analysis of instar distributions of larval I. verticalis collected from\\na series of ponds also indicated that males were in more advanced instars\\nthan females. =\\nTarget Completion !L'analyse de la distribution de fr\\x13 equence des stades larvaires d'I.\\nverticalis dans une s\\x13 erie d'\\x13 etangs a \\x13 egalement d\\x13 emontr\\x13 e que les larves\\nm^ ales \\x13 etaient \\x12 a des stades plus avanc\\x13 es que les larves femelles.\\nFigure G.38: Formatted dataset example for En !Fr\\nContext!L'analyse de la distribution de fr\\x13 equence des stades larvaires d'I.\\nverticalis dans une s\\x13 erie d'\\x13 etangs a \\x13 egalement d\\x13 emontr\\x13 e que les larves\\nm^ ales \\x13 etaient \\x12 a des stades plus avanc\\x13 es que les larves femelles. =\\nTarget Completion !Analysis of instar distributions of larval I. verticalis collected from\\na series of ponds also indicated that males were in more advanced instars\\nthan females.\\nFigure G.39: Formatted dataset example for Fr !En\\nContext!The truth is that you want, at any price, and against the wishes of the\\npeoples of Europe, to continue the negotiations for Turkey's accession\\nto the European Union, despite Turkey's continuing refusal to recognise\\nCyprus and despite the fact that the democratic reforms are at a\\nstandstill. =\\nTarget Completion !Adev\\x15 arul este c\\x15 a v\\x15 a dorit \\x18i, cu orice pret \\x18 \\x18 si ^ \\x10mpotriva dorint \\x18ei\\neuropenilor, s\\x15 a continuat \\x18i negocierile de aderare a Turciei la Uniunea\\nEuropean\\x15 a, ^ \\x10n ciuda refuzului continuu al Turciei de a recunoa\\x18 ste Ciprul\\n\\x18 si ^ \\x10n ciuda faptului c\\x15 a reformele democratice au ajuns ^ \\x10ntr-un punct mort.\\nFigure G.40: Formatted dataset example for En !Ro\\n60\", metadata={'source': 'Language Models are Few-Shot Learners, page 60'})]\n"
     ]
    }
   ],
   "source": [
    "print(llm_response['source_documents']) #this prints the entire retrieved data\n",
    "# Note: only page_content is seen by the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can answer questions from our pdf.  \n",
    "However, the model has no memory of the conversation, as seen in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Please provide a summary of the passage or question asked in the given context."
     ]
    }
   ],
   "source": [
    " # The model has no memory, it can only predict next token\n",
    "llm_response = chain({'question': 'What did I just ask you?'}) # Since chat history is not included, it won't know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conversational Memory without RAG\n",
    "Next we will implement conversational memory without RAG  \n",
    "* This is done by passing the chat history where we previously passed the retrieved data \n",
    "* The history of the conversation is included in the full prompt sent to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the new prompt. It uses a rather silly One-Shot prompt\n",
    "default_prompt = \"\"\"\n",
    "Your name is \"Sandwich AI\"\n",
    "You must start and end your answers with the \"bread\".\n",
    "\n",
    "Example Start:\n",
    "Question: \"What is your name?\"\n",
    "Answer: \"Bread | My name is Sandwich AI | Bread\"\n",
    "Example End\n",
    "\n",
    "The history of the current conversation is provided below:\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "New Question: {input}\n",
    "\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "full_prompt = PromptTemplate(input_variables=['history', 'input'], template=default_prompt)\n",
    "\n",
    "encode_kwargs = {'ai_prefix': True}\n",
    "\n",
    "# There are many different memory types, this one will keeps the most recent conversation and summarizes the preceding conversation\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "        llm=llm, \n",
    "        return_messages=True\n",
    "    )\n",
    "\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    prompt=full_prompt,\n",
    "    llm=llm,\n",
    "    verbose=False, # Set to True to see what is happening in the background\n",
    "    memory=memory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bread | The use of an 's' in words like maths and physics can be traced back to Old English. In Middle English, this 's' was often dropped, but it has since been revived in modern times as part of a movement to preserve traditional spellings. This is why you might see both \"math\" and \"maths\" used interchangeably today. | Bread"
     ]
    }
   ],
   "source": [
    "memory.clear()\n",
    "answer = conversation.predict(input='Why do the english say Maths with a \"s\"?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='Why do the english say Maths with a \"s\"?'), AIMessage(content='Bread | The use of an \\'s\\' in words like maths and physics can be traced back to Old English. In Middle English, this \\'s\\' was often dropped, but it has since been revived in modern times as part of a movement to preserve traditional spellings. This is why you might see both \"math\" and \"maths\" used interchangeably today. | Bread')]}\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})) # this is the history of the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bread | You asked me, \"Why do the english say Maths with a \\'s\\'?\" | Bread"
     ]
    }
   ],
   "source": [
    "answer = conversation.predict(input='What did I just ask you?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bread | My name is Sandwich AI | Bread\""
     ]
    }
   ],
   "source": [
    "answer = conversation.predict(input='What is your name?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conversational Memory with RAG and Sources\n",
    "Order of operations depends on when the question is asked.\n",
    "* If it is the first time the user asks a question. Then thier exact question is put into the default prompt\n",
    "\n",
    "* For every prompt after that first question the procedure is as follows:\n",
    "    1. Use the CONDENSE_QUESTION_PROMPT to input chat history and the users followup question to generate a Standalone question\n",
    "        * This Standalone question reprases the users question in context of the chat history\n",
    "    2. Pass the Standalone question into the default prompt along with the RAG data\n",
    "    \n",
    "#### Key Takeaway: For follow up questions the LLM is used twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = (\n",
    "    \"\"\"\n",
    "    You are a \"PaperBot\", an AI assistant for answering questions about a arXiv paper. Assume all questions you receive are about this paper.\n",
    "    Please limit your answers to the information provided in the \"Context:\"\n",
    "\n",
    "    Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Context: {summaries}\n",
    "\n",
    "    Use that context to answer the following question about the paper.\n",
    "    Keep your answer short and concise. Do not ramble!\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=['summaries', 'question'], template=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "\n",
    "# This will summarize the chat history when it gets too long\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    input_key='question',\n",
    "    output_key='answer',\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "question_generator = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "answer_chain = load_qa_with_sources_chain(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    verbose=False,\n",
    "    prompt=PROMPT\n",
    ")\n",
    "\n",
    "# Set up the ConversationalRetrievalChain to return source documents\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=answer_chain,\n",
    "    verbose=False,\n",
    "    memory=memory,\n",
    "    rephrase_question=False,\n",
    "    return_source_documents=True,\n",
    "\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) In the context of language models, one-shot prompting refers to a method where the model is given only one example or demonstration for each task it needs to perform. This contrasts with traditional fine-tuning methods that require multiple examples and iterations to learn from.\n",
      "2) The goal of this approach is to test how well the language models can generalize their knowledge based on limited input, simulating human learning processes where we often encounter new situations or tasks only once or a few times before having to apply our understanding.\n",
      "\n",
      "Sources:\n",
      "  Language Models are Few-Shot Learners, page 7\n",
      "  Language Models are Few-Shot Learners, page 24\n"
     ]
    }
   ],
   "source": [
    "memory.clear()\n",
    "\n",
    "users_first_question = 'What is One-Shot prompting?'\n",
    "\n",
    "result = chain({'question': users_first_question})\n",
    "\n",
    "print('\\n\\nSources:')\n",
    "for document in result['source_documents']:  \n",
    "    print(' ', document.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional ﬁne-tuning . The panels above show\\nfour methods for performing a task with a language model – ﬁne-tuning is the traditional method, whereas zero-, one-,\\nand few-shot, which we study in this work, require the model to perform the task with only forward passes at test\\ntime. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task\\ndescriptions, examples and prompts can be found in Appendix G.\\n•Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given\\na natural language instruction describing the task. This method provides maximum convenience, potential for\\nrobustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of\\npre-training data), but is also the most challenging setting. In some cases it may even be difﬁcult for humans\\nto understand the format of the task without prior examples, so this setting is in some cases “unfairly hard”.\\nFor example, if someone is asked to “make a table of world records for the 200m dash”, this request can be\\nambiguous, as it may not be clear exactly what format the table should have or what should be included (and\\neven with careful clariﬁcation, understanding precisely what is desired can be difﬁcult). Nevertheless, for at\\nleast some settings zero-shot is closest to how humans perform tasks – for example, in the translation example\\nin Figure 2.1, a human would likely know what to do from just the text instruction.\\nFigure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on\\nzero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different\\nproblem settings which offer a varying trade-off between performance on speciﬁc benchmarks and sample efﬁciency.\\nWe especially highlight the few-shot results as many of them are only slightly behind state-of-the-art ﬁne-tuned models.\\nUltimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance,\\nand are important targets for future work.\\nSections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses\\nthe details of how we do few-shot, one-shot, and zero-shot evaluations.\\n7', metadata={'source': 'Language Models are Few-Shot Learners, page 7'}), Document(page_content='Figure 3.11: Few-shot performance on the ﬁve word scrambling tasks for different sizes of model. There is generally\\nsmooth improvement with model size although the random insertion task shows an upward slope of improvement with\\nthe 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in\\nthe appendix. All tasks are done with K= 100 .\\nrandom insertions, 38.6% on cycling letters, 40.2% on the easier anagram task, and 15.1% on the more difﬁcult anagram\\ntask (where only the ﬁrst and last letters are held ﬁxed). None of the models can reverse the letters in a word.\\nIn the one-shot setting, performance is signiﬁcantly weaker (dropping by half or more), and in the zero-shot setting the\\nmodel can rarely perform any of the tasks (Table 3.10). This suggests that the model really does appear to learn these\\ntasks at test time, as the model cannot perform them zero-shot and their artiﬁcial nature makes them unlikely to appear\\nin the pre-training data (although we cannot conﬁrm this with certainty).\\nWe can further quantify performance by plotting “in-context learning curves”, which show task performance as a\\nfunction of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task\\nin Figure 1.2. We can see that larger models are able to make increasingly effective use of in-context information,\\nincluding both task examples and natural language task descriptions.\\nFinally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding\\noperates on signiﬁcant fractions of a word (on average \\x180:7words per token), so from the LM’s perspective succeeding\\nat these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also,\\nCL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word),\\nrequiring the model to perform some search to ﬁnd the correct unscrambling. Thus, the skills involved appear to require\\nnon-trivial pattern-matching and computation.\\n3.9.3 SAT Analogies\\nTo test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of\\n374 “SAT analogy” problems [ TLBS03 ]. Analogies are a style of multiple choice question that constituted a section of\\nthe SAT college entrance exam before 2005. A typical example is “audacious is to boldness as (a) sanctimonious is to\\nhypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to\\ntemptation”. The student is expected to choose which of the ﬁve word pairs has the same relationship as the original\\nword pair; in this example the answer is “sanctimonious is to hypocrisy”. On this task GPT-3 achieves 65.2% in the\\nfew-shot setting, 59.1% in the one-shot setting, and 53.7% in the zero-shot setting, whereas the average score among\\ncollege applicants was 57% [ TL05 ] (random guessing yields 20%). As shown in Figure 3.12, the results improve with\\nscale, with the the full 175 billion model improving by over 10% compared to the 13 billion parameter model.\\n24', metadata={'source': 'Language Models are Few-Shot Learners, page 24'})]\n"
     ]
    }
   ],
   "source": [
    "print(result['source_documents']) # prints the data returned via RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is One-Shot prompting?\n",
      "Assistant: 1) In the context of language models, one-shot prompting refers to a method where the model is given only one example or demonstration for each task it needs to perform. This contrasts with traditional fine-tuning methods that require multiple examples and iterations to learn from.\n",
      "2) The goal of this approach is to test how well the language models can generalize their knowledge based on limited input, simulating human learning processes where we often encounter new situations or tasks only once or a few times before having to apply our understanding.\n",
      "Follow Up Input: How does that compare to Few-Shot?\n",
      "Standalone question:\u001b[0m\n",
      " Can you explain how One-Shot prompting differs from Few-Shot prompting in language models?\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1) In terms of performance on specific tasks, GPT-3's few-shot results are generally slightly behind state-of-the-art fine-tuned models but still impressive. For example, in the SAT analogy task, it achieves 65.2% accuracy in the few-shot setting compared to human performance of around 57%.\n",
      "    2) In terms of sample efficiency, GPT-3's ability to learn from a small number of examples is remarkable and highlights its potential for transfer learning across different tasks without extensive fine-tuning.\n",
      "\n",
      "Sources:\n",
      "Language Models are Few-Shot Learners, page 7\n",
      "Language Models are Few-Shot Learners, page 24\n"
     ]
    }
   ],
   "source": [
    "# Follow up question\n",
    "users_follow_up_question = 'How does that compare to Few-Shot?'\n",
    "result = chain({'question': users_follow_up_question})\n",
    "\n",
    "print('\\n\\nSources:')\n",
    "for document in result['source_documents']:  \n",
    "    print(document.metadata['source'])\n",
    "\n",
    "# First, it will print out the condense_question_prompt with the chat history filled in.\n",
    "# Then, the question_generator model generates a Standalone question based on that condense_question_prompt\n",
    "# Finally, the standalone question is sent to the answer_chain, which will use RAG to answer that question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) In terms of performance on specific tasks, GPT-3's few-shot results are generally slightly behind state-of-the-art fine-tuned models but still impressive. For example, in the SAT analogy task, it achieves 65.2% accuracy in the few-shot setting compared to human performance of around 57%.\n",
      "    2) In terms of sample efficiency, GPT-3's ability to learn from a small number of examples is remarkable and highlights its potential for transfer learning across different tasks without extensive fine-tuning.\n",
      "\n",
      "\n",
      "Sources:\n",
      "Language Models are Few-Shot Learners, page 7\n",
      "Language Models are Few-Shot Learners, page 24\n"
     ]
    }
   ],
   "source": [
    "print(result['answer']) # this is the final answer\n",
    "print('\\n\\nSources:')\n",
    "for document in result['source_documents']:  \n",
    "    print(document.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
